---
title: "edX MovieLens Submittal"
author: "Eduardo Caballero"
date: "2/26/2022"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}


#Movielens Data

##########################################################
# Create edx set, validation set (final hold-out test set)
##########################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)


ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")


# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                            title = as.character(title),
                                            genres = as.character(genres))


movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
      semi_join(edx, by = "movieId") %>%
      semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

############################################
#### End of Initial Code Given
###########################################

###################################################
#### Install any additional libraries to use
###################################################

if(!require(dslabs)) install.packages("dslabs", repos = "http://cran.us.r-project.org")
if(!require(broom)) install.packages("broom", repos = "http://cran.us.r-project.org")
if(!require(gtools)) install.packages("gtools", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("ddplyr", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
if(!require(skimr)) install.packages("skimr", repos = "http://cran.us.r-project.org")
if(!require(rpart)) install.packages("rpart", repos = "http://cran.us.r-project.org")
if(!require(ranger)) install.packages("ranger", repos = "http://cran.us.r-project.org")



library(dslabs)
library(broom)
library(gtools)
library(dplyr)
library(ggplot2)
library(lubridate)
library(randomForest)
library(skimr)
library(rpart)
library(ranger)






#################################################################
#### Additional columns added to data to be used in evaluation
################################################################# 


# Create variable to calculate the percent of times a user rates a half point, i.e 2.5, 3.5, 4.5
# value of rate_half is 0 if a whole number, 1 if it has a half increment

edx <- edx %>% mutate(rate_half = (abs(round(rating,0) - rating)*2))

# Group by userId to create user specific data, such as what percentage of time do they rate a half point
# what their average rating and standard deviation are and how many ratings they have down

user_info <- edx %>% group_by(userId) %>% summarize(user_pct_half = mean(rate_half), userAvg = mean(rating), userStd = sd(rating), userReviews = n())
str(user_info)

#Group by movieId to create movie specific data

movie_info <- edx %>% group_by(movieId) %>% summarize(MovieAvg = mean(rating), MovieStd = sd(rating), MovieReviews = n())
str(movie_info)


#Add Columns to datasets for consideration

edx <- left_join(edx, user_info, by = "userId")
edx <- left_join(edx, movie_info, by = "movieId")

validation <- left_join(validation, user_info, by = "userId")
validation <- left_join(validation, movie_info, by = "movieId")



###########################################
####Reviewing Data
###########################################

# dimensions of edx data
dim(edx)
str(edx)

# dimensions of validation data
dim(validation)

# Create a smaller sample to look at distribution ( Original Dataset was too large to plot distribution)
ind <- edx$userId %>% sample(1000000, replace = FALSE)
edx_dist <- edx[ind]



#####################################
# Create a test and train dataset
#####################################

#Split the edx dataset into a 20% test and a 80% train dataset


set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
edxtest_index <- createDataPartition(y = edx$rating, times = 1, p = 0.2, list = FALSE)
edx_train <- edx[-edxtest_index,]
edx_temp <- edx[edxtest_index,]

# Make sure userId and movieId in test set are also in train set
edx_test <- edx_temp %>% 
      semi_join(edx_train, by = "movieId") %>%
      semi_join(edx_train, by = "userId")

# Add rows removed from test set back into train set
edx_removed <- anti_join(edx_temp, edx_test)
edx_train <- rbind(edx_train, edx_removed)

rm(edxtest_index, edx_temp, edx_removed)



# Create smaller samples to test code when the whole data set is too large to compute

set.seed(1, sample.kind="Rounding")
ind <- edx_train$userId %>% sample(10000, replace = FALSE)
edx10k <- edx_train[ind]

set.seed(1, sample.kind="Rounding")
ind <- edx_train$userId %>% sample(100000, replace = FALSE)
edx100k <- edx_train[ind]

set.seed(1, sample.kind="Rounding")
ind <- edx_train$userId %>% sample(1000000, replace = FALSE)
edx1M <- edx_train[ind]



#########################################################
### Train the model
#########################################################


#Select method pick predictors (user_pct_half, userAvg, userStd, MovieAvg, MovieStd, MovieReviews)

# Set the size sample for the training data set
train_set <- edx1M

###################knn model with edx10k,edx100k Test_RMSE = 0.8832128, 0.9454087

#control <- trainControl(method = "cv")
#model_knn <- train(rating ~user_pct_half + userAvg + MovieAvg, data = train_set, method = "knn", tuneGrid = data.frame(k = seq(30, 50, 2)), trControl = control)
#model_knn


################## glm model with edx10k,edx100k, edx1M Test_RMSE =0.8714957, 0.8716486, 0.8716314 crashed at 10M

model_glm <- train(rating ~user_pct_half + userAvg + MovieAvg, data = train_set, method = "glm")
mglm <- model_glm


################### Random Forest model with edx10k, Test_RMSE =0.9591408

#control <- trainControl(method = "cv", number = 10)
#grid <- data.frame(mtry = c(1, 5, 10))
#model_rf <- train(rating ~ user_pct_half + userAvg + MovieAvg, data = train_set, method = "rf", 
#	ntree = 20,
#	trControl = control,
#	tuneGrid = grid,
#	nSamp = 5000)


##################### rpart with edx10k, edx100k, edx1M Test_RMSE =  0.8945104, 0.8916313, 0.8918748

#getModelInfo("rpart")
#modelLookup("rpart")


set.seed(1, sample.kind="Rounding")
model_rpart <- train(
  rating ~ user_pct_half + userAvg + MovieAvg, data = train_set, method = "rpart",
  trControl = trainControl("cv", number = 10),
  tuneLength = 20
  )
mrpart <- model_rpart

# Plot model error vs different values of cp (complexity parameter)
mrplot <- plot(model_rpart)

# Print the best tuning parameter cp that minimize the model RMSE
mrcp <- model_rpart$bestTune

# Print the decision tree
plot(model_rpart$finalModel, margin = 0.1)
text(model_rpart$finalModel, cex = 0.75)



################### Ranger with edx10k, Test_RMSE =  0.9073114

#model_ranger <- train(rating ~user_pct_half + userAvg + MovieAvg, data = train_set, method = "ranger")
#model_ranger



```

## Executive Summary 
The objective of this project was to develop a model to predict the rating a user would give a movie. The data set used was the movielens dataset with 10 million observations. Code was provided to download the files and split the data into a validation set and a training set.
The dimensions and structure of the data set was as given below:

```{r, echo = FALSE}
str(edx)
head(edx, 5)
```

The training dataset was subdivided into a training and testing set.  The training set consisted of 80% of the data.The data was then reviewed and a modeling approach was developed. The breakdown of unique users and movies is given below.
```{r, echo = FALSE}

# Summarize the # of users and movies in the data
unique_summary <- edx %>% summarize(unique_users = n_distinct(userId), unique_movies= n_distinct(movieId))
unique_summary

```


Various models were tested using the caret library. Models tested included knn, glm, random forest, rpart and ranger. The root mean squared error (RMSE) was utilized to determine the best model. The glm model gave the best results of the models tested.



##  Analysis 
section that explains the process and techniques used, including data cleaning, data exploration and visualization, insights gained, and your modeling approach

As part of the analysis the data was reviewed to determine what the best approach would be.  A sample of the dataset was taken to determine the distribution of the ratings.

```{r, echo = FALSE}

#Plot a distribution of ratings
edx_dist %>% 
ggplot(aes(rating)) +
geom_histogram(binwidth = 0.5, fill = "blue", col = "black") +
xlab("Rating Given") +
ggtitle("Distribution of Ratings")



```
As can be seen in the graph, ratings were mostly 3's and 4's with much fewer 3.5's or 4.5's. In order to determine how an individual might rate a movie, additional columns based on past ratings by the user were developed.  Columns for the mean rating a user gave, standard deviation of the user's rating and the percentage of times that a user would give a rating with a half point such as 0.5, 1.5, 2.5, 3.5 or 4.5.This percent was calculated as the rounded rating to zero decimal points minus the rating.  If a whole number rating was given this would evaluate to a 0 and if a rating with a half was given it would evaluate to a -.05 or 0.5.  This value was then doubled and the absolute value taken so that the final value was a 0 if a whole number rating was given or a 1 if a rating with a half was given.  The average value over all ratings was then taken to determine the percent of time the user would rate on a half. This variable was named user_pct_half


Other columns were added to get the average rating a specific movie ,MovieAvg, had and the standard deviations ,MovieStd, of that average and also the number of reviews ,MovieReviews, the movie had. This was also done for each user creating columns named userAvg ,userStd and userReviews.

The skimr package was then used to get a quick summary of the data as shown below.The data was not missing fields except for the Movie standard deviations which was for movies that only had one rating.  Because of this preprocessing of the data was unnecessary.
```{r, echo = FALSE}
# Quick Analysis of the columns to check for missing data

skimmed <- skim(edx)
skimmed[,1:7]
```

The data file created was very large for the processing power of my laptop. So in order to run the models a random sample of the data was taken to run the models on a smaller data set. Data sets of 10,000, 100,000 and 1 million were created and the models were run progressively until the laptop would freeze. 

##  Results 
Various machine learning models were tested to determine which one gave the best predicted results. Models tested were knn, glm, random forest, rpart and ranger. The models were tested on increasingly sized data sets. The predictors used to train the model were user_pct_half + userAvg + MovieAvg

The knn model was run with the default cross validation. The model was run with 10,000 and 100,000 records to train the model. Once a trained model was established the model was tested on the test data set.  The RMSE results for the 10,000 model was 0.8832128 and when ran with the 100,000 data set RMSE was 0.9454087


The Random Forest model was run with cross validation.  The model could only be run with 10,000 data set before the laptop would crash. The RMSE results for the 10,000 model was 0.9591408

The rpart model was run with cross validation. The model was run with data sets 10,000, 100,000 and 1,000,000. The RMSE results for the 10,000 model was 0.8945104, when ran with the 100,000 data set RMSE was 0.8916313 and with the 1,000,000 data set RMSE was 0.8918748. The model information is shown below.

```{r, echo = FALSE}
mrpart

# Plot model error vs different values of cp (complexity parameter)
mrplot

# Print the best tuning parameter cp that minimize the model RMSE
mrcp

# Print the decision tree
plot(model_rpart$finalModel, margin = 0.1)
text(model_rpart$finalModel, cex = 0.75)
```



The glm model was run with data sets 10,000, 100,000 and 1,000,000. The RMSE results for the 10,000 model was 0.8714957, when ran with the 100,000 data set RMSE was 0.8716486 and with the 1,000,000 data set RMSE was 0.8716314. The model information is shown below.

```{r, echo = FALSE}
mglm
```

Since this model provided the best RMSE on the test data. The model was used on the validation data set to determine how well it predicted unknown ratings
The RMSE for the Validation set is shown below.
```{r, echo = FALSE, include = FALSE}


##################################################
######Test the method on the test data
##################################################

#Create function to do a Residual Mean Square Error

RMSE <- function(true_ratings, predicted_ratings){
     sqrt(mean((true_ratings - predicted_ratings)^2))
}


#select the model to use

f_model <-model_glm

rating_hat <- predict(f_model, edx_test)

Test_RMSE <- RMSE(edx_test$rating, rating_hat)

Test_RMSE


```

```{r, echo = FALSE}

###################################################
############# Final Check
###################################################

ratingPrediction <- predict(f_model, validation)
Validation_RMSE <- RMSE(validation$rating,ratingPrediction)
Validation_RMSE
```

##  Conclusion

Based on the models run the glm model performed the best. With more processing power on a computer the other models could have been tested with larger data sets which may have given better results.  Also looking for ways to make the code more efficient may have helped with the processing limitations that were encountered.  Additional fine tuning of all the model parameters could be attempted in the future as well as testing additional models to find a better performing model.
